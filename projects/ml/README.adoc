=== Activation Functions

==== Sigmoid function

The sigmoid function $\sigma(x) = \frac{1}{1 + e^{-x}}$ is commonly used in neural networks. The range of the sigmoid function( $\sigma(x)$ ) is $[0, 1]$.

[source,math]
----
\begin{aligned}
\sigma(x) &= \frac{1}{1 + e^{-x}} \\
\sigma'(x) &= \sigma(x)\left(1 - \sigma(x)\right)
\end{aligned}
----

[source,rust]
----
pub fn sigmoid<T: num_traits::Float>(x: T) -> T {
    T::one() / (T::one() + (-x).exp())
}

pub fn sigmoid_derivative<T: num_traits::Float>(x: T) -> T {
    let s = sigmoid(x);
    s * (T::one() - s)
}
----

=== Hyperbolic tangent function and its derivative

The range of the tanh function is $[-1, 1]$.

[source,math]
----
\begin{aligned}
\tanh(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
\frac{d}{dx} \tanh(x) &= 1 - \tanh^2(x)
\end{aligned}
----

[source,rust]
----
pub fn tanh<T: num_traits::Float>(x: T) -> T {
    x.tanh()
}

pub fn tanh_derivative<T: num_traits::Float>(x: T) -> T {
    let t = x.tanh();
    T::one() - t * t
}
----

=== ReLU function and its derivative

[source,math]
----
\begin{aligned}
\mathrm{ReLU}(x) &= \max(0, x) \\
\frac{d}{dx} \mathrm{ReLU}(x) &= 
\begin{cases}
1 & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
\end{aligned}
----

[source,rust]
----
pub fn relu<T>(x: T) -> T
where
    T: core::cmp::PartialOrd + num_traits::identities::Zero,
{
    if x > T::zero() { x } else { T::zero() }
}

pub fn relu_derivative<T>(x: T) -> T
where
    T: core::cmp::PartialOrd + num_traits::identities::Zero + num_traits::identities::One,
{
    if x > T::zero() { T::one() } else { T::zero() }
}
----

=== Identity function and its derivative

[source,math]
----
\begin{aligned}
f(x) &= x \\
f'(x) &= 1
\end{aligned}
----

[source,rust]
----
// Identity function (does nothing)
pub fn identity<T>(x: T) -> T {
    x
}

// The derivative of the identity function is always 1
pub fn identity_derivative<T: num_traits::identities::One>(_: T) -> T {
    T::one()
}
----
